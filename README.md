# Universal LLM API Call

Universal LLM API Call is a FastAPI-based application designed to provide a unified interface for interacting with Large Language Models (LLMs) from various vendors, including OpenAI and Google. This application simplifies the process of generating and streaming responses from LLMs by abstracting vendor-specific API calls into a single, easy-to-use API.

## Features

- **Unified API**: Interact with multiple LLM vendors (e.g., OpenAI, Google) using a single API.
- **Synchronous and Asynchronous Support**: Generate responses synchronously or stream responses asynchronously.
- **Customizable System Prompt**: Define a system prompt via environment variables to guide the behavior of the LLM.
- **CORS Support**: Built-in Cross-Origin Resource Sharing (CORS) middleware for secure API access.
- **Error Handling**: Graceful handling of API errors with detailed error messages.

## Requirements

- Python 3.12 or higher
- FastAPI
- Uvicorn
- OpenAI Python SDK
- Python dotenv

## Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/hadi2525/universal-llm-api-call.git
   cd universal-llm-api-call

2. Create a virtual environment:
   ```bash
   python -m venv venv
   ```
3. Activate the virtual environment:
   - On Windows:
     ```bash
     venv\Scripts\activate
     ```
   - On macOS/Linux:
     ```bash
     source venv/bin/activate
     ```
4. Install the required packages:
   ```bash
   pip install -r requirements.txt
   ```
5. Set up environment variables:
   Create a `.env` file in the root directory of the project and add your API keys:
   ```env
   OPENAI_API_KEY=your_openai_api_key
   GOOGLE_API_KEY=your_google_api_key
   SYSTEM_PROMPT=your_system_prompt
   ```
   Replace `your_openai_api_key`, `your_google_api_key`, and `your_system_prompt` with your actual API keys and desired system prompt.

## Usage
1. Start the FastAPI server:
   ```bash
   uvicorn main:app --reload
   ```
2. Access the API documentation at `http://localhost:8000/docs` to explore the available endpoints and test the API.
3. Use the `/generate/response` endpoint to generate responses from the LLM. You can specify the vendor (e.g., `openai`, `google`) and the prompt in your API request.
4. For streaming responses, use the `/stream/response` endpoint. This will provide a real-time stream of tokens as they are generated by the LLM.
5. For synchronous responses, use the `/sync/response` endpoint. This will return the complete response once it is generated.
6. For asynchronous responses, use the `/async/response` endpoint. This will return the complete response once it is generated, but allows for other tasks to be performed while waiting for the response.
7. For error handling, the API will return appropriate error messages for invalid requests or API errors.
## Example API Requests
### Generate Response
```bash
curl -X POST "http://localhost:8000/generate/response" \
-H "Content-Type: application/json" \
-d '{
  "vendor": "openai",
  "prompt": "What is the capital of France?"
}'
```
### Stream Response
```bash
curl -X POST "http://localhost:8000/stream/response" \
-H "Content-Type: application/json" \
-d '{
  "vendor": "google",
  "prompt": "What is the capital of France?"
}'
```
### Sync Response
```bash
curl -X POST "http://localhost:8000/sync/response" \
-H "Content-Type: application/json" \
-d '{
  "vendor": "openai",
  "prompt": "What is the capital of France?"
}'
```
### Async Response
```bash
curl -X POST "http://localhost:8000/async/response" \
-H "Content-Type: application/json" \
-d '{
  "vendor": "google",
  "prompt": "What is the capital of France?"
}'
```
## License
This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.
## Contributing
Contributions are welcome! If you have suggestions for improvements or new features, please open an issue or submit a pull request.
  